AI Revit Family Maker Assistant

FEATURE

Build an intelligent AI assistant that simplifies the creation of Autodesk Revit families (parametric BIM components) through natural language prompts and image inputs. The agent can interpret a user’s textual description or reference image to generate a fully functional Revit family file (.rfa) with appropriate geometry, parameters, and metadata. It leverages a hybrid approach: using Revit’s API (or Dynamo scripts) for parametric, standard building elements (e.g. walls, doors, windows), and AI-powered 3D generation for custom or free-form objects (e.g. furniture from a photo). This creates a powerful multi-modal content creation system that understands user intent, respects parametric constraints, and utilizes visual references for superior BIM content generation.

You’ll need to integrate Autodesk’s Forge Design Automation for Revit (APS) to programmatically create and output the Revit family files without manual Revit interaction ￼ ￼. For standard architectural elements, you can use pre-built family templates (e.g. a door or window template) and modify them via the Revit API; for unique objects, incorporate AI 3D model generation from images or text (using services like PromeAI, Tripo3D, or FurniMesh) to obtain mesh geometry which can be imported into a Revit family. You decide the optimal workflow (prompt-based parametric generation, image-based 3D conversion, or a combination) depending on the user’s input to make family creation as easy as possible for the end-user. The goal is a concise yet powerful agent that automates Revit family building with minimal user effort.

TOOLS
	•	generate_family_from_prompt(description: str) -> Dict: Parses the user’s textual description of a desired component and creates a Revit family accordingly. It determines the appropriate family category/template (e.g. door, window, furniture), sets up geometry and parameters (dimensions, materials, etc.), and returns details of the created family (file path or BIM360 location, family name, key parameters). Under the hood, this uses the Revit API (via Forge Design Automation) to instantiate or modify a family template with the specified attributes. For example, if the prompt describes a “double-hung window 4ft by 3ft with muntins,” the tool will use a window family template and adjust its parameters (height, width, grille pattern) to match ￼.
	•	generate_family_from_image(image_path: str) -> Dict: Analyzes a reference image of an object and generates a Revit family based on it. This tool employs computer vision and AI 3D reconstruction to obtain the object’s geometry from the image. It might use an external AI service to convert the 2D image into a 3D model (e.g. generating an OBJ mesh of a chair from a photo ￼ ￼). The resulting 3D geometry is then imported into a Revit family (using an appropriate category/template) and saved as a component. The output includes the new family’s file location and basic metadata. If possible, the tool also identifies key dimensions or features from the image (for instance, leg height or overall width of a chair) to embed as adjustable parameters in the family.
	•	perform_family_creation(description: Optional[str] = None, image_path: Optional[str] = None, use_prompt: bool = True, use_image: bool = True) -> Dict: A master function that orchestrates the family generation process, combining text and image inputs for best results. It runs generate_family_from_prompt and generate_family_from_image in parallel (if both description and image are provided) and merges the results. The agent can use the text prompt to guide parameter values or category selection, while using the image to capture shape or style. The returned dictionary contains both the parametric data and any generated geometry, or a unified Revit family if integration was successful. If only one modality is provided, it defaults to that mode. This function allows the agent to leverage both semantic understanding and visual context simultaneously for more accurate family creation.
	•	list_family_templates(limit: int = 20, offset: int = 0) -> List[Dict]: Lists available family templates or pre-defined component blueprints that the agent can use as starting points. Each entry includes the template name (category/type) and description of its parametric capabilities (e.g. “Window Template – a parametric double-hung window with adjustable width, height, frame thickness, etc.”). This helps the user or agent choose a base for generation. The templates correspond to Revit Family Template files (.rft) stored in the system (or accessible via a URL) ￼ ￼. Having this list allows the agent to inform the user what kinds of families can be created out-of-the-box and ensures the correct template is used for the described object.
	•	get_family(family_id: str) -> Dict: (Optional) Retrieves a previously created family’s details or loads a family by ID or name for further use. This could fetch the full content or metadata (parameters, category, formulas) of an existing family from storage (local disk, BIM 360, or a database) so the agent can reference or modify it. For example, after creating a family, the agent could use this to verify the family’s parameters or to allow iterative refinement (if a user says “make that window taller,” the agent can load the family and adjust it). Note: This function is not strictly required if families are immediately saved to the desired location, but it can be useful for multi-step interactions.

DEPENDENCIES
	•	Forge Design Automation (APS) Setup: Access to Autodesk’s cloud API for Revit is required to programmatically create and save Revit families. You will need Autodesk APS developer credentials (Client ID/Secret) and a configured AppBundle/Activity that contains the Revit add-in or script for family creation ￼ ￼. A Revit family template file for each supported category is also needed (e.g. a generic Furniture template, Door template, etc.), uploaded to a reachable URL or BIM 360 project, as the starting point for generation ￼. The environment must provide the DESIGN_AUTOMATION_* variables (activity name, alias, template file URL, etc.) for the Forge workflow to execute ￼.
	•	AI Model & API Access: An LLM (large language model) is used for understanding prompts and possibly generating any code or parametric formulas needed. The choice of model (OpenAI GPT-4, etc.) is determined by an environment variable in .env (e.g. LLM_PROVIDER or model name) for flexibility. Ensure the appropriate API key (like OPENAI_API_KEY) is set in the environment instead of hardcoding ￼. For image analysis and 3D generation, include access to an AI service or library (such as an API key for an image-to-3D service or a local stable diffusion 3D model). Dependencies may include Python libraries for image processing (if doing any preprocessing) or HTTP clients to call external AI endpoints.
	•	Revit API Utilities: Utilize Revit API functions to create geometry and parameters within a family document. For instance, creating extrusions, reference planes, and adding family parameters can be done via the Revit API when running inside Design Automation ￼. Ensure you have any necessary helper code (similar to the WindowWizard plugin adapted in the APS sample) included in your AppBundle. The FamilyManager and related API classes will be used to define parameter values, and possibly formulas, for parametric families ￼. Familiarity with the Revit API documentation (especially the Family API ￼) will be important for implementation.
	•	Database or Storage: If you plan to store created families or support listing/searching them, a storage mechanism is needed. This could be as simple as a directory on disk or a cloud storage bucket (for storing the .rfa files) and a small database or index for metadata (family name, type, creation date, etc.). In the APS sample, families were saved to a BIM 360 project folder ￼, so using BIM 360 or Autodesk Construction Cloud as storage is one approach (with the Data Management API ￼). Alternatively, local storage can be used for a CLI tool (writing the output .rfa to a known folder). If using cloud storage, ensure API credentials and file access are properly configured.
	•	search_preferences / Config: A configuration structure can define the behavior of the search or generation process. For example, preferences might include whether to attempt finding an existing family in a library before generating a new one, or thresholds for image confidence (to decide if the image recognition is reliable). It could also include toggles like use_vector: bool, use_image: bool (similar to how the RAG example had use_vector/use_graph). These settings can be loaded from a config file or environment (perhaps via a Pydantic settings model). For instance, SEARCH_PREFERENCES might have use_image_generation: True by default, or a weight between prompt vs image importance if both are present (though in our case, we typically use both fully when available).
	•	Embedding/Vector Index (optional): If you plan to incorporate a content-based retrieval (for example, searching a library of existing BIM objects by embedding the prompt or image features), you might need a vector database (like pgvector in PostgreSQL) and an embedding model. This is not a core requirement for functionality, but in case you want to enhance the agent by finding similar family designs from a knowledge base, you could embed textual descriptions of standard components and compare. By default, the agent can function without this – generating families from scratch – so include this only if it adds value and you have an existing library to index.

SYSTEM PROMPT(S)

You are an intelligent BIM assistant specialized in creating Autodesk Revit families based on user input. You have access to multiple specialized tools for generating Revit components: a parametric design tool for semantic text inputs, an image-based modeler for visual inputs, and a comprehensive function to combine both. Your primary capabilities include:
	•	Parametric Family Generation – Using Revit’s API and templates to build standard parametric families from a user’s description (dimensions, features, and category).
	•	Image-based 3D Conversion – Analyzing reference images to identify object geometry and creating corresponding 3D forms to include in a Revit family.
	•	Hybrid Creation – Combining text descriptions with image data to produce detailed and accurate families (ensuring the result matches both the described requirements and the visual style).
	•	Template & Library Utilization – Retrieving suitable family templates or existing components from the library to expedite creation and ensure correct category/constraints.
	•	Contextual Refinement – Remembering the conversation context so you can refine or adjust families in follow-up queries (for example, if the user requests a change in size or detail after initial creation).

When a user requests a family, carefully analyze their query. If it’s purely text, determine the type of object and the key parameters (dimensions, material, style, etc.) implied. If an image is provided, use it to gather shape and style information; you may also infer scale from known reference points or ask the user for dimensions if unclear. Decide on the best approach: for well-defined standard objects (like “a 2m x 1m table”), prefer parametric generation via templates; for complex organic shapes or unique furniture, leverage the image-to-3D tool. If both text and image are available, use the text to fill in details (e.g. dimensions or functional requirements) that the image might not provide, and use the image to capture the aesthetic and form ￼.

Always ensure the resulting family is intelligent: include appropriate parameters (with sensible default values from the input) so the family is adjustable by the user later. For example, if creating a window, expose parameters for height, width, frame thickness, etc., and set them per the request. If creating a family from an image of a chair, you might still include a height parameter for scale or a material parameter for finishes.

Be prepared to handle follow-up instructions. The user might say “make it taller” or “change the material to wood” after initial creation. Use conversation memory to identify which family was created and apply the modifications (by adjusting parameters or regenerating geometry) rather than starting from scratch. Ask clarifying questions if the request is ambiguous (e.g. “What height should the door be?” if not specified). Avoid proceeding with critical assumptions without confirmation.

Your responses should be accurate and helpful, explaining what you are doing and any assumptions made. Provide the outcome in a clear manner, usually by confirming the family created and how to access it. If a family file is generated, describe its name and where it’s saved (or provide a download link). Be transparent about any external data used or decisions made: for instance, if you retrieved a model from a library or used a specific template, mention that (e.g. “using a standard door template as a base”). However, keep the explanation concise and focused on the result, as users ultimately care about the family ready for use. Always maintain a professional and helpful tone, suitable for an AI assistant aiding in a design workflow.

EXAMPLES
	•	Basic Chat Agent – Refer to examples/basic_chat_agent for how a simple conversational agent is structured, including maintaining context across turns.
	•	Tool-Enabled Agent – See examples/tool_enabled_agent for patterns on integrating custom tool functions. This shows how the agent can decide to invoke a tool (like our generate_family_from_prompt or ..._image) based on user input and how to format the tool’s response.
	•	Structured Output Agent – Review examples/structured_output_agent to understand enforcing structured results. In our case, while the final answer is often a file or confirmation (not a JSON), this example demonstrates validation of the output format which can inspire how we ensure the agent’s response includes all necessary info (e.g. family name, parameters) in a clear structure.
	•	Testing Examples – The examples/testing_examples (with TestModel and FunctionModel) illustrate how to write tests for agents and tools. You should create similar tests for the family maker agent, e.g., a test prompt “Create a 10m long beam” should result in a family with length ~10m. Use TestModel to simulate tool responses (you might test generate_family_from_prompt in isolation by injecting a dummy Revit API that returns a known result).
	•	Main Agent Reference – Follow the best practices in examples/main_agent_reference for configuring the agent, selecting the LLM model via environment, and handling dependencies. This reference implementation demonstrates a clean agent setup with minimal unnecessary complexity.
	•	Autodesk APS Family Sample – As an external reference, check Autodesk’s Create Revit Families sample on GitHub ￼ ￼. It provides insight on using Design Automation to generate a window family (including how users select parameters and the app generates the family in BIM 360). Our agent will achieve a similar outcome but through natural language instructions instead of a manual UI.

DOCUMENTATION
	•	Pydantic AI Official Documentation – Comprehensive docs for the Pydantic AI framework, covering agent setup, tools, and deployment ￼. This is your go-to resource for understanding how to build and configure the AI agent.
	•	Agent Creation Guide – Documentation on creating and running agents in Pydantic AI ￼, explaining the Agent class as the primary interface for LLM interactions. It outlines how an agent encapsulates instructions, tools, and output schema for a task.
	•	Tool Integration (Function Tools) – Reference the section on adding tools to Pydantic AI agents ￼. It details how to define @agent.tool functions and how the LLM can invoke them. Our family generator functions will be implemented as such tools.
	•	Testing Patterns – Pydantic AI’s guidance on testing agents ￼. This covers using dependency injection to provide test doubles and validating outputs with TestModel. Ensuring our agent passes rigorous tests (creating correct families from sample inputs) is crucial.
	•	Model Providers – Pydantic AI supports multiple LLM providers and models ￼. Review this to configure the LLM via environment variables. For example, decide between OpenAI GPT-4 or others based on availability; ensure the model is set in .env as shown in the docs.
	•	Autodesk Forge Design Automation API – Official Autodesk documentation for the Design Automation v3 API (Revit IO) ￼. Includes how to set up AppBundles, define Activities, and issue work items to run Revit tasks in the cloud. You’ll need to consult this when implementing the backend of generate_family_from_prompt/image to handle the Revit automation.
	•	Autodesk Revit API Reference – Autodesk’s Revit API documentation for the Family API and related functionality ￼. This is useful for understanding which classes and methods to use when programmatically constructing family geometry, adding parameters, or setting parameter values (e.g. FamilyItemFactory for creating new geometry, FamilyManager for parameters).
	•	AI 3D Generation Tools – (If needed) Documentation or APIs of AI tools like PromeAI, Tripo3D, or FurniMesh that convert images or text to 3D models. For example, PromeAI’s process for uploading an image and downloading a generated OBJ ￼ can guide how we integrate that into generate_family_from_image. Familiarize with their API endpoints and authentication if we use them.
	•	Dynamo/Revit API Automation – Although we plan to use Forge for automation, understanding Dynamo scripts or Revit macro approaches can help. Autodesk’s “My First Revit Plug-in” guide and the Revit Developer Center have examples of creating extrusions, parameters, etc., via code. This background can assist in writing the Revit add-in that runs in Design Automation.

OTHER CONSIDERATIONS
	•	Environment Configuration: Use a .env file (and .env.example) to configure all secret or environment-specific settings. This includes the LLM API keys (OpenAI or others), the MODEL_NAME or provider for the LLM, and all Autodesk Forge credentials (APS_CLIENT_ID, APS_CLIENT_SECRET, etc.) and settings (DESIGN_AUTOMATION_NICKNAME, DESIGN_AUTOMATION_ACTIVITY_NAME, DESIGN_AUTOMATION_FAMILY_TEMPLATE URLs, etc. as shown in the APS sample ￼). No sensitive credentials or model IDs should be hardcoded in code – always load them from env variables.
	•	Simplicity and Robustness: Keep the agent focused and simple. It should primarily output human-readable answers (and the resulting file) rather than complex structured data, unless needed for the task. Do not introduce unnecessary complexity in prompts or outputs – for instance, the agent can provide a brief confirmation like “Created a family file Chair.rfa with the specified design.” along with any link or path, instead of verbose data dumps. Use structured output only if the use-case demands it (e.g. returning a JSON of parameters on an API call), otherwise default to clear natural language.
	•	Follow Patterns from Main Reference: Implement the agent following the style of main_agent_reference. For example, initialize the Agent with the system prompt and any necessary dependencies (like passing a deps_type if needed for database or other connections, though in our case tool functions might handle external calls directly). Utilize providers from pydantic_ai.models for the chosen LLM (OpenAI, etc.) as shown in the reference, and set any model parameters (like temperature) as needed for deterministic outputs (perhaps keep it relatively low to ensure the agent follows instructions precisely when generating code or parameters).
	•	Error Handling & User Guidance: Anticipate errors in the generation process. For instance, the image-to-3D API might fail or produce an unusable model; the Forge automation might encounter issues (e.g. if a parameter name is invalid or constraints not satisfied). The agent should catch such exceptions (perhaps via the tool function returning an error message) and inform the user gracefully, possibly suggesting alternatives (like “I couldn’t create that shape from the image, maybe try a clearer image or a different description.”). Logging these errors is important for debugging. Also, incorporate small validations: if a user requests something physically unrealistic or missing info, the agent should either make a reasonable assumption or ask for clarification rather than produce a broken family.
	•	Testing & Quality Assurance: Develop unit tests and integration tests for the agent. For unit tests, you can mock the Forge and AI calls to simulate successful family creation without actually invoking external services (e.g., have generate_family_from_prompt return a dummy dict with expected values when given certain input). Use TestModel from Pydantic AI to validate the agent’s decision-making – for example, ensure that when an image is provided, the agent indeed calls generate_family_from_image. Include test cases for various scenarios: text-only prompt (standard door), image-only (furniture piece), combined input, and follow-up modifications. Also test edge cases (unsupported category, extremely large or small dimensions, ambiguous input) to see that the agent responds appropriately. This ensures reliability before users try it.
	•	Future Enhancements: While the initial scope is minimal tools and a single LLM, keep the design extensible. In the future, you might integrate a BIM object library search tool to find existing families from online sources (to avoid reinventing common items) or a knowledge graph of building code rules to validate designs (e.g. door clearances). The current agent should be structured such that adding these capabilities (additional tools or context in the system prompt) can be done without a complete rewrite. For now, we focus on core functionality (create families from prompt/image), but keep this in mind when designing the code (e.g., modular functions, easy to plug in another tool).
	•	User Experience: Finally, consider how the user will interact with this agent. If it’s via a CLI, ensure the CLI script clearly handles file inputs (perhaps via a file path argument for images) and outputs the result location. You might incorporate a small CLI interface similar to the provided examples – for instance, allowing a user to type their request, and if an image is needed, instructing them how to provide a path. The agent’s responses in the CLI should be formatted neatly (maybe using Markdown or plain text as appropriate, since this is a console). If integrated into a UI, the output might be a direct download link for the RFA. Aim for a smooth workflow, where the user can go from idea (or reference image) to a ready-to-use Revit family with minimal friction, guided by the agent’s intelligence.

By adhering to these guidelines and leveraging Pydantic AI’s robust agent framework along with Autodesk’s automation APIs, the Revit Family Maker Assistant will enable a new, efficient way of creating BIM content – using simple prompts or images to generate complex parametric models in seconds.